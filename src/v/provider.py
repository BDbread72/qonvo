"""
Gemini ì±„íŒ… í”„ë¡œë°”ì´ë”
"""
import os
import base64
import time
import threading
from typing import List, Generator
from dataclasses import dataclass

from google import genai
from google.genai import types


# ëª¨ë¸ ì •ì˜ (ID â†’ í‘œì‹œ ì´ë¦„)
MODELS = {
    "gemini-3.1-pro-preview": "Gemini 3.1 Pro",
    "gemini-3-pro-preview": "Gemini 3.0 Pro",
    "gemini-3-flash-preview": "Gemini 3.0 Flash",
    "gemini-2.5-pro": "Gemini 2.5 Pro",
    "gemini-2.5-flash": "Gemini 2.5 Flash",
    "gemini-3-pro-image-preview": "ğŸŒNanobanana Pro",
    "gemini-2.5-flash-image": "ğŸŒNanobanana",
    "imagen-4.0-generate-001": "ğŸ¨ Imagen 4",
    "imagen-4.0-ultra-generate-001": "ğŸ¨ Imagen 4 Ultra",
    "imagen-4.0-fast-generate-001": "ğŸ¨ Imagen 4 Fast",
}

# UIìš© ëª¨ë¸ ID ëª©ë¡
MODEL_IDS = list(MODELS.keys())

# ê³µí†µ ìƒì„± ì˜µì…˜ (temperature, top_p, max_output_tokens)
_COMMON_GEN_OPTIONS = {
    "temperature": {
        "type": "float",
        "label": "Temperature",
        "min": 0.0,
        "max": 2.0,
        "step": 0.05,
        "default": 1.0,
    },
    "top_p": {
        "type": "float",
        "label": "Top P",
        "min": 0.0,
        "max": 1.0,
        "step": 0.05,
        "default": 0.95,
    },
    "max_output_tokens": {
        "type": "int",
        "label": "Max Tokens",
        "min": 1,
        "max": 65536,
        "default": 8192,
    },
}

# ì´ë¯¸ì§€ ëª¨ë¸ìš© (max_output_tokens ì œì™¸)
_IMAGE_GEN_OPTIONS = {
    "temperature": _COMMON_GEN_OPTIONS["temperature"],
    "top_p": _COMMON_GEN_OPTIONS["top_p"],
}

# ë¹„ìœ¨ ì˜µì…˜ (ì´ë¯¸ì§€ ëª¨ë¸ ê³µí†µ)
_ASPECT_RATIO_OPTION = {
    "aspect_ratio": {
        "type": "choice",
        "label": "ë¹„ìœ¨",
        "values": ["1:1", "16:9", "9:16", "4:3", "3:4"],
        "default": "1:1",
    },
}

# ëª¨ë¸ë³„ ì˜µì…˜ ìŠ¤í‚¤ë§ˆ (UI ë™ì  ìƒì„±ìš©)
MODEL_OPTIONS = {
    "gemini-3.1-pro-preview": {
        "thinking_level": {
            "type": "choice",
            "label": "Thinking",
            "values": ["HIGH", "MEDIUM", "LOW"],
            "default": "HIGH",
        },
        **_COMMON_GEN_OPTIONS,
    },
    "gemini-3-pro-preview": {
        "thinking_level": {
            "type": "choice",
            "label": "Thinking",
            "values": ["HIGH", "MEDIUM", "LOW"],
            "default": "HIGH",
        },
        **_COMMON_GEN_OPTIONS,
    },
    "gemini-3-flash-preview": {
        "thinking_level": {
            "type": "choice",
            "label": "Thinking",
            "values": ["HIGH", "MEDIUM", "LOW"],
            "default": "HIGH",
        },
        **_COMMON_GEN_OPTIONS,
    },
    "gemini-2.5-pro": {
        "thinking_budget": {
            "type": "int",
            "label": "Thinking Budget",
            "min": 0,
            "max": 24576,
            "default": 2804,
        },
        **_COMMON_GEN_OPTIONS,
    },
    "gemini-2.5-flash": {
        "thinking_budget": {
            "type": "int",
            "label": "Thinking Budget",
            "min": 0,
            "max": 24576,
            "default": 0,
        },
        **_COMMON_GEN_OPTIONS,
    },
    "gemini-3-pro-image-preview": {
        **_ASPECT_RATIO_OPTION,
        **_IMAGE_GEN_OPTIONS,
    },
    "gemini-2.5-flash-image": {
        **_ASPECT_RATIO_OPTION,
        **_IMAGE_GEN_OPTIONS,
    },
    "imagen-4.0-generate-001": {
        **_ASPECT_RATIO_OPTION,
    },
    "imagen-4.0-ultra-generate-001": {
        **_ASPECT_RATIO_OPTION,
    },
    "imagen-4.0-fast-generate-001": {
        **_ASPECT_RATIO_OPTION,
    },
}


def get_default_options(model: str) -> dict:
    """ëª¨ë¸ì˜ ê¸°ë³¸ ì˜µì…˜ ë°˜í™˜ (ë‚´ì¥ + í”ŒëŸ¬ê·¸ì¸)"""
    from v.model_plugin import get_all_model_options
    schema = get_all_model_options().get(model, {})
    if not schema:
        return {}
    return {
        key: opt["default"]
        for key, opt in schema.items()
        if "default" in opt
    }


@dataclass
class ChatMessage:
    """ì±„íŒ… ë©”ì‹œì§€"""
    role: str  # "user" or "assistant"
    content: str
    attachments: List[str] = None  # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ëª©ë¡
    thought_signatures: list = None  # Gemini 3 ë©€í‹°í„´ ì‹œ í•„ìˆ˜ (ëª¨ë¸ ì‘ë‹µ íŒŒíŠ¸ë³„ ì„œëª…)


class GeminiProvider:
    """Gemini API í”„ë¡œë°”ì´ë” (ë‹¤ì¤‘ í‚¤ ë¼ìš´ë“œ ë¡œë¹ˆ ì§€ì›)"""

    def __init__(self, api_key: str = None, api_keys: list[str] = None):
        if api_keys:
            self._api_keys = list(api_keys)
        else:
            single = api_key or os.environ.get("GEMINI_API_KEY")
            self._api_keys = [single] if single else []

        self.api_key = self._api_keys[0] if self._api_keys else None
        self._cancel_requested = False
        self._clients: list[genai.Client | None] = [None] * len(self._api_keys)
        self._client_index = 0
        self._lock = threading.Lock()

    @property
    def key_count(self) -> int:
        return len(self._api_keys)

    def _get_client(self) -> genai.Client:
        """í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ (ë¼ìš´ë“œ ë¡œë¹ˆ, thread-safe)"""
        if not self._api_keys:
            raise RuntimeError("API key is required")

        with self._lock:
            idx = self._client_index
            self._client_index = (idx + 1) % len(self._api_keys)

        if self._clients[idx] is None:
            try:
                self._clients[idx] = genai.Client(api_key=self._api_keys[idx])
            except Exception:
                # P1: ìƒì„± ì‹¤íŒ¨ ì‹œ ìºì‹œì— ë‚¨ê¸°ì§€ ì•ŠìŒ
                self._clients[idx] = None
                raise
        return self._clients[idx]

    def invalidate_client(self, key_index: int = -1):
        """P1: ìºì‹œëœ í´ë¼ì´ì–¸íŠ¸ ë¬´íš¨í™” (ì¸ì¦ ì‹¤íŒ¨ ì‹œ í˜¸ì¶œ)"""
        with self._lock:
            if key_index < 0:
                self._clients = [None] * len(self._api_keys)
            elif 0 <= key_index < len(self._clients):
                self._clients[key_index] = None

    def _get_client_with_index(self) -> tuple:
        """í´ë¼ì´ì–¸íŠ¸ + ì‚¬ìš©ëœ key index ë°˜í™˜ (batch job ì¶”ì ìš©)"""
        if not self._api_keys:
            raise RuntimeError("API key is required")

        with self._lock:
            idx = self._client_index
            self._client_index = (idx + 1) % len(self._api_keys)

        if self._clients[idx] is None:
            self._clients[idx] = genai.Client(api_key=self._api_keys[idx])
        return self._clients[idx], idx

    def _get_client_at(self, key_index: int) -> genai.Client:
        """íŠ¹ì • API key indexë¡œ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (batch resumeìš©)"""
        if key_index < 0 or key_index >= len(self._api_keys):
            raise IndexError(f"Key index {key_index} out of range (have {len(self._api_keys)} keys)")
        if self._clients[key_index] is None:
            self._clients[key_index] = genai.Client(api_key=self._api_keys[key_index])
        return self._clients[key_index]

    def _get_safety_settings(self) -> list:
        """ê³µí†µ ì•ˆì „ ì„¤ì • (ëª¨ë‘ BLOCK_NONE)"""
        return [
            types.SafetySetting(
                category="HARM_CATEGORY_HARASSMENT",
                threshold="BLOCK_NONE",
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_HATE_SPEECH",
                threshold="BLOCK_NONE",
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
                threshold="BLOCK_NONE",
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_DANGEROUS_CONTENT",
                threshold="BLOCK_NONE",
            ),
        ]

    _MIME_MAP = {
        "png": "image/png", "jpg": "image/jpeg",
        "jpeg": "image/jpeg", "gif": "image/gif",
        "webp": "image/webp", "bmp": "image/bmp",
        "pdf": "application/pdf",
    }
    _SKIP_SIG = b"skip_thought_signature_validator"

    def _build_system_instruction(self, text, files):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸/íŒŒì¼ â†’ system_instruction Content ë¹Œë“œ"""
        if not text and not files:
            return None
        parts = []
        for fpath in (files or []):
            if not os.path.isfile(fpath):
                continue
            ext = fpath.lower().rsplit(".", 1)[-1] if "." in fpath else ""
            if ext in self._MIME_MAP:
                try:
                    with open(fpath, "rb") as f:
                        parts.append(types.Part.from_bytes(
                            data=f.read(),
                            mime_type=self._MIME_MAP[ext],
                        ))
                except Exception as e:
                    from v.logger import get_logger
                    logger = get_logger("qonvo.provider")
                    logger.warning(f"Failed to read file {fpath}: {e}")
            else:
                try:
                    with open(fpath, "r", encoding="utf-8") as f:
                        content = f.read()
                    fname = os.path.basename(fpath)
                    parts.append(types.Part.from_text(text=f"[{fname}]:\n{content}"))
                except Exception as e:
                    from v.logger import get_logger
                    logger = get_logger("qonvo.provider")
                    logger.warning(f"Failed to read text file {fpath}: {e}")
        if text:
            parts.append(types.Part.from_text(text=text))
        return types.Content(parts=parts) if parts else None

    def _convert_messages(self, messages: List[ChatMessage]) -> list:
        """ChatMessage â†’ types.Content ë³€í™˜

        ëª¨ë¸ ì‘ë‹µ íŒŒíŠ¸ëŠ” ë°˜ë“œì‹œ thought_signatureë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤.
        ì €ì¥ëœ ì„œëª…ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë³µì›, ì—†ìœ¼ë©´ skip validatorë¡œ fallback.
        """
        contents = []
        for msg in messages:
            parts = []
            is_model = (msg.role == "assistant")

            if is_model:
                # â”€â”€ ëª¨ë¸ ì‘ë‹µ: ì„œëª… ë³µì› (í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ìˆœì„œ) â”€â”€
                decoded_sigs = []
                for s in (msg.thought_signatures or []):
                    if isinstance(s, str):
                        try:
                            decoded_sigs.append(base64.b64decode(s))
                        except Exception:
                            decoded_sigs.append(None)
                    elif isinstance(s, bytes):
                        decoded_sigs.append(s)
                    else:
                        decoded_sigs.append(None)

                sig_idx = 0
                def _next_sig():
                    nonlocal sig_idx
                    sig = decoded_sigs[sig_idx] if sig_idx < len(decoded_sigs) else None
                    sig_idx += 1
                    return sig if sig is not None else self._SKIP_SIG

                # í…ìŠ¤íŠ¸ íŒŒíŠ¸ (í•­ìƒ ì²« ë²ˆì§¸)
                parts.append(types.Part(
                    text=msg.content,
                    thought_signature=_next_sig(),
                ))

                # ì²¨ë¶€ íŒŒì¼ íŒŒíŠ¸
                if msg.attachments:
                    for fpath in msg.attachments:
                        ext = fpath.lower().rsplit(".", 1)[-1] if "." in fpath else ""
                        if ext in self._MIME_MAP:
                            try:
                                with open(fpath, "rb") as f:
                                    file_bytes = f.read()
                                parts.append(types.Part(
                                    inline_data=types.Blob(
                                        data=file_bytes,
                                        mime_type=self._MIME_MAP[ext],
                                    ),
                                    thought_signature=_next_sig(),
                                ))
                            except Exception as e:
                                from v.logger import get_logger
                                logger = get_logger("qonvo.provider")
                                logger.warning(f"Failed to read attachment {fpath}: {e}")
                        else:
                            try:
                                with open(fpath, "r", encoding="utf-8") as f:
                                    content = f.read()
                                fname = os.path.basename(fpath)
                                parts.append(types.Part(
                                    text=f"[{fname}]:\n{content}",
                                    thought_signature=_next_sig(),
                                ))
                            except Exception as e:
                                from v.logger import get_logger
                                logger = get_logger("qonvo.provider")
                                logger.warning(f"Failed to read text attachment {fpath}: {e}")
            else:
                # â”€â”€ ìœ ì € ë©”ì‹œì§€: ì„œëª… ë¶ˆí•„ìš” â”€â”€
                if msg.attachments:
                    pass  # attachments processing
                    for fpath in msg.attachments:
                        ext = fpath.lower().rsplit(".", 1)[-1] if "." in fpath else ""
                        fname = os.path.basename(fpath)
                        pass  # per-attachment
                        if ext in self._MIME_MAP:
                            try:
                                with open(fpath, "rb") as f:
                                    file_bytes = f.read()
                                pass  # file read ok
                                parts.append(types.Part.from_text(text=f"[{fname}]:"))
                                parts.append(types.Part.from_bytes(
                                    data=file_bytes,
                                    mime_type=self._MIME_MAP[ext],
                                ))
                            except Exception as e:
                                from v.logger import get_logger
                                logger = get_logger("qonvo.provider")
                                logger.warning(f"Failed to read user file {fpath}: {e}")
                                pass  # logged above
                        else:
                            try:
                                with open(fpath, "r", encoding="utf-8") as f:
                                    content = f.read()
                                parts.append(types.Part.from_text(
                                    text=f"[{fname}]:\n{content}"))
                            except Exception as e:
                                from v.logger import get_logger
                                logger = get_logger("qonvo.provider")
                                logger.warning(f"Failed to read user text file {fpath}: {e}")
                parts.append(types.Part.from_text(text=msg.content))

            role = "model" if is_model else msg.role
            contents.append(types.Content(role=role, parts=parts))
        return contents

    def chat(
        self,
        model: str,
        messages: List[ChatMessage],
        stream: bool = True,
        **options
    ) -> Generator[str, None, None] | str:
        """
        ì±„íŒ… ìš”ì²­ (ëª¨ë¸ë³„ ë¶„ê¸°)
        - model: ëª¨ë¸ ID
        - messages: ëŒ€í™” ê¸°ë¡
        - stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
        - **options: ëª¨ë¸ë³„ ì¶”ê°€ ì˜µì…˜ (thinking_level, thinking_budget, aspect_ratio ë“±)
        """
        self._cancel_requested = False

        # í”ŒëŸ¬ê·¸ì¸ ëª¨ë¸ ë””ìŠ¤íŒ¨ì¹˜
        from v.model_plugin import PluginRegistry
        plugin = PluginRegistry.instance().get_plugin_for_model(model)
        if plugin:
            options.pop("system_prompt", None)
            options.pop("system_files", None)
            return plugin.chat(model, messages, stream, **options)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ (ë‚´ì¥ ëª¨ë¸ìš©)
        sys_text = options.pop("system_prompt", "")
        sys_files = options.pop("system_files", [])
        options["_sys_instr"] = self._build_system_instruction(sys_text, sys_files)

        # ëª¨ë¸ë³„ ë©”ì„œë“œ ë¶„ê¸°
        if model in ("gemini-3.1-pro-preview", "gemini-3-pro-preview"):
            return self._chat_gemini_3_pro(messages, stream, _model_id=model, **options)
        elif model == "gemini-3-flash-preview":
            return self._chat_gemini_3_flash(messages, stream, **options)
        elif model == "gemini-2.5-pro":
            return self._chat_gemini_25_pro(messages, stream, **options)
        elif model == "gemini-2.5-flash":
            return self._chat_gemini_25_flash(messages, stream, **options)
        elif model == "gemini-3-pro-image-preview":
            return self._chat_nanobanana_pro(messages, stream, **options)
        elif model == "gemini-2.5-flash-image":
            return self._chat_nanobanana(messages, stream, **options)
        elif model.startswith("imagen-4.0-"):
            return self._chat_imagen(model, messages, stream, **options)
        else:
            raise ValueError(f"Unknown model: {model}")

    def cancel(self):
        """ì§„í–‰ ì¤‘ì¸ ìš”ì²­ ì·¨ì†Œ"""
        self._cancel_requested = True

    # ============================================================
    # Gemini 3.0 Pro
    # ============================================================
    def _chat_gemini_3_pro(
        self,
        messages: List[ChatMessage],
        stream: bool,
        **options
    ) -> Generator[str, None, None] | str:
        """Gemini 3.x Pro ì±„íŒ… (thinking ì§€ì›, 3.0/3.1 ê³µìš©)"""
        model_id = options.pop("_model_id", "gemini-3-pro-preview")
        client = self._get_client()
        contents = self._convert_messages(messages)

        thinking_level = options.get("thinking_level", "HIGH")

        config = types.GenerateContentConfig(
            system_instruction=options.get("_sys_instr"),
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level,
            ),
            temperature=options.get("temperature"),
            top_p=options.get("top_p"),
            max_output_tokens=options.get("max_output_tokens"),
            safety_settings=self._get_safety_settings(),
        )

        if stream:
            return self._stream_with_signatures(model_id, contents, config)
        else:
            response = client.models.generate_content(
                model=model_id,
                contents=contents,
                config=config,
            )
            return response.text

    # ============================================================
    # Gemini 3.0 Flash
    # ============================================================
    def _chat_gemini_3_flash(
        self,
        messages: List[ChatMessage],
        stream: bool,
        **options
    ) -> Generator[str, None, None] | str:
        """Gemini 3.0 Flash ì±„íŒ… (thinking ì§€ì›)"""
        client = self._get_client()
        contents = self._convert_messages(messages)

        thinking_level = options.get("thinking_level", "HIGH")

        config = types.GenerateContentConfig(
            system_instruction=options.get("_sys_instr"),
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level,
            ),
            temperature=options.get("temperature"),
            top_p=options.get("top_p"),
            max_output_tokens=options.get("max_output_tokens"),
            safety_settings=self._get_safety_settings(),
        )

        if stream:
            return self._stream_with_signatures("gemini-3-flash-preview", contents, config)
        else:
            response = client.models.generate_content(
                model="gemini-3-flash-preview",
                contents=contents,
                config=config,
            )
            return response.text

    # ============================================================
    # Gemini 2.5 Pro
    # ============================================================
    def _chat_gemini_25_pro(
        self,
        messages: List[ChatMessage],
        stream: bool,
        **options
    ) -> Generator[str, None, None] | str:
        """Gemini 2.5 Pro ì±„íŒ… (thinking budget ì§€ì›)"""
        client = self._get_client()
        contents = self._convert_messages(messages)

        thinking_budget = options.get("thinking_budget", 2804)

        config = types.GenerateContentConfig(
            system_instruction=options.get("_sys_instr"),
            thinking_config=types.ThinkingConfig(
                thinking_budget=thinking_budget,
            ),
            temperature=options.get("temperature"),
            top_p=options.get("top_p"),
            max_output_tokens=options.get("max_output_tokens"),
            safety_settings=self._get_safety_settings(),
        )

        if stream:
            return self._stream_with_signatures("gemini-2.5-pro", contents, config)
        else:
            response = client.models.generate_content(
                model="gemini-2.5-pro",
                contents=contents,
                config=config,
            )
            return response.text

    # ============================================================
    # Gemini 2.5 Flash
    # ============================================================
    def _chat_gemini_25_flash(
        self,
        messages: List[ChatMessage],
        stream: bool,
        **options
    ) -> Generator[str, None, None] | str:
        """Gemini 2.5 Flash ì±„íŒ… (thinking budget ì§€ì›)"""
        client = self._get_client()
        contents = self._convert_messages(messages)

        thinking_budget = options.get("thinking_budget", 0)

        config = types.GenerateContentConfig(
            system_instruction=options.get("_sys_instr"),
            thinking_config=types.ThinkingConfig(
                thinking_budget=thinking_budget,
            ),
            temperature=options.get("temperature"),
            top_p=options.get("top_p"),
            max_output_tokens=options.get("max_output_tokens"),
            safety_settings=self._get_safety_settings(),
        )

        if stream:
            return self._stream_with_signatures("gemini-2.5-flash", contents, config)
        else:
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=contents,
                config=config,
            )
            return response.text

    # ============================================================
    # Nanobanana Pro (ì´ë¯¸ì§€ ìƒì„±)
    # ============================================================
    def _chat_nanobanana_pro(
        self,
        messages: List[ChatMessage],
        stream: bool,  # ë¯¸ì‚¬ìš© (ì´ë¯¸ì§€ ìƒì„±ì€ ë™ê¸°ì‹ë§Œ)
        **options
    ) -> dict:
        """
        ğŸŒNanobanana Pro ì´ë¯¸ì§€ ìƒì„±
        ë°˜í™˜: {"text": str, "images": [bytes, ...]}
        ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› (ì´ë¯¸ì§€ ìƒì„±ì€ ë™ê¸°ì‹ë§Œ)
        """
        _ = stream  # unused
        client = self._get_client()
        contents = self._convert_messages(messages)

        aspect_ratio = options.get("aspect_ratio", "1:1")

        config = types.GenerateContentConfig(
            system_instruction=options.get("_sys_instr"),
            response_modalities=["TEXT", "IMAGE"],
            image_config=types.ImageConfig(
                aspect_ratio=aspect_ratio,
            ),
            temperature=options.get("temperature"),
            top_p=options.get("top_p"),
            safety_settings=self._get_safety_settings(),
        )

        response = client.models.generate_content(
            model="gemini-3-pro-image-preview",
            contents=contents,
            config=config,
        )

        return self._parse_image_response(response)

    # ============================================================
    # Nanobanana (ì´ë¯¸ì§€ ìƒì„±)
    # ============================================================
    def _chat_nanobanana(
        self,
        messages: List[ChatMessage],
        stream: bool,  # ë¯¸ì‚¬ìš© (ì´ë¯¸ì§€ ìƒì„±ì€ ë™ê¸°ì‹ë§Œ)
        **options
    ) -> dict:
        """
        ğŸŒNanobanana ì´ë¯¸ì§€ ìƒì„±
        ë°˜í™˜: {"text": str, "images": [bytes, ...]}
        ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› (ì´ë¯¸ì§€ ìƒì„±ì€ ë™ê¸°ì‹ë§Œ)
        """
        _ = stream  # unused
        client = self._get_client()
        contents = self._convert_messages(messages)

        aspect_ratio = options.get("aspect_ratio", "1:1")

        config = types.GenerateContentConfig(
            system_instruction=options.get("_sys_instr"),
            response_modalities=["TEXT", "IMAGE"],
            image_config=types.ImageConfig(
                aspect_ratio=aspect_ratio,
            ),
            temperature=options.get("temperature"),
            top_p=options.get("top_p"),
            safety_settings=self._get_safety_settings(),
        )

        response = client.models.generate_content(
            model="gemini-2.5-flash-image",
            contents=contents,
            config=config,
        )

        return self._parse_image_response(response)

    # ============================================================
    # Imagen 4.0 (ìˆœìˆ˜ ì´ë¯¸ì§€ ìƒì„±)
    # ============================================================
    def _chat_imagen(
        self,
        model: str,
        messages: List[ChatMessage],
        stream: bool,
        **options
    ) -> dict:
        """
        ğŸ¨ Imagen 4.0 ì´ë¯¸ì§€ ìƒì„±
        generate_images API ì‚¬ìš© (ë©€í‹°í„´ ë¯¸ì§€ì›, ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©)

        Phase 4: ì¥ì‹œê°„ ì‘ì—… ì‹œê°„ ë¡œê¹… ì¶”ê°€
        """
        _ = stream
        client = self._get_client()

        # Imagenì€ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ (text-to-image ì „ìš©) â€” ê²½ê³  ì¶œë ¥
        has_images = any(msg.attachments for msg in messages if msg.attachments)
        if has_images:
            from v.logger import get_logger
            logger = get_logger("qonvo.provider")
            logger.warning(
                "Imagen does not support reference images (text-to-image only). "
                "Use Nanobanana Pro for image-to-image tasks."
            )

        # ë§ˆì§€ë§‰ ìœ ì € ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
        prompt = ""
        for msg in reversed(messages):
            if msg.role == "user" and msg.content:
                prompt = msg.content
                break
        if not prompt:
            return {"text": "", "images": [], "thought_signatures": []}

        aspect_ratio = options.get("aspect_ratio", "1:1")

        config_dict = {
            "number_of_images": 1,
            "output_mime_type": "image/jpeg",
            "person_generation": "ALLOW_ADULT",
            "aspect_ratio": aspect_ratio,
        }
        # image_sizeëŠ” fast ëª¨ë¸ì—ì„œ ë¯¸ì§€ì›
        if "fast" not in model:
            config_dict["image_size"] = "1K"

        # Phase 4: ì´ë¯¸ì§€ ìƒì„± ì†Œìš” ì‹œê°„ ë¡œê¹…
        try:
            from v.logger import get_logger
            logger = get_logger("qonvo.provider")
            logger.info(f"Image generation started: {prompt[:50]}... (model: {model})")
        except:
            logger = None

        start_time = time.time()
        result = client.models.generate_images(
            model=f"models/{model}",
            prompt=prompt,
            config=config_dict,
        )
        elapsed = time.time() - start_time

        if logger:
            logger.info(f"Image generation completed in {elapsed:.2f}s")

        if not result.generated_images:
            return {"text": "", "images": [], "thought_signatures": []}

        images = []
        for generated_image in result.generated_images:
            images.append(generated_image.image.image_bytes)

        return {
            "text": "",
            "images": images,
            "thought_signatures": [],
        }

    def _stream_with_signatures(self, model, contents, config):
        """ìŠ¤íŠ¸ë¦¬ë° + thought_signatures ìˆ˜ì§‘ ê³µí†µ ë©”ì„œë“œ"""
        def stream_gen():
            usage = None
            sigs = []
            stream_error = None
            try:
                for chunk in self._get_client().models.generate_content_stream(
                    model=model,
                    contents=contents,
                    config=config,
                ):
                    if self._cancel_requested:
                        break
                    if chunk.text:
                        yield chunk.text
                    if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                        usage = chunk.usage_metadata
                    # thought_signatures ìˆ˜ì§‘ (thinking íŒŒíŠ¸ ì œì™¸)
                    if hasattr(chunk, 'candidates') and chunk.candidates:
                        for part in (chunk.candidates[0].content.parts or []):
                            if getattr(part, 'thought', False):
                                continue
                            sig = getattr(part, 'thought_signature', None)
                            if sig:
                                encoded = base64.b64encode(sig).decode('ascii') if isinstance(sig, bytes) else sig
                                sigs.append(encoded)
            except Exception as e:
                # P2: ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìˆ˜ì§‘ëœ ë©”íƒ€ë°ì´í„° ì „ë‹¬
                stream_error = e
            # ë©”íƒ€ë°ì´í„°ëŠ” ì˜¤ë¥˜ ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ ì „ë‹¬
            if usage:
                yield {"__usage__": True,
                       "prompt_tokens": getattr(usage, 'prompt_token_count', 0),
                       "candidates_tokens": getattr(usage, 'candidates_token_count', 0)}
            if sigs:
                yield {"__thought_signatures__": sigs}
            if stream_error:
                yield {"__error__": str(stream_error)}
        return stream_gen()

    def _parse_image_response(self, response) -> dict:
        """ì´ë¯¸ì§€ ìƒì„± ì‘ë‹µ íŒŒì‹±"""
        result = {"text": "", "images": [], "thought_signatures": []}

        # ì‘ë‹µ ì°¨ë‹¨(ê²€ì—´) ê°ì§€
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            finish_reason = getattr(candidate, 'finish_reason', None)
            if finish_reason and str(finish_reason) not in ('STOP', 'FinishReason.STOP', 'MAX_TOKENS', 'FinishReason.MAX_TOKENS'):
                reason_str = str(finish_reason)
                pass  # blocked
                result["text"] = f"[ì‘ë‹µ ì°¨ë‹¨ë¨: {reason_str}]"
                return result
        if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
            block_reason = getattr(response.prompt_feedback, 'block_reason', None)
            if block_reason:
                reason_str = str(block_reason)
                pass  # prompt blocked
                result["text"] = f"[í”„ë¡¬í”„íŠ¸ ì°¨ë‹¨ë¨: {reason_str}]"
                return result

        if not (response.parts or []):
            pass  # empty response

        text_sig = None   # ë§ˆì§€ë§‰ í…ìŠ¤íŠ¸ íŒŒíŠ¸ì˜ ì„œëª… (ë‹¤ì¤‘ í…ìŠ¤íŠ¸ íŒŒíŠ¸ ëŒ€ì‘)
        image_sigs = []   # ì´ë¯¸ì§€ë³„ ì„œëª…

        for part in (response.parts or []):
            # thinking íŒŒíŠ¸ ìŠ¤í‚µ
            if getattr(part, 'thought', False):
                continue
            sig = getattr(part, 'thought_signature', None)
            # bytes â†’ base64 ë¬¸ìì—´ (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ)
            if isinstance(sig, bytes):
                sig = base64.b64encode(sig).decode('ascii')
            if part.text:
                result["text"] += part.text
                text_sig = sig   # ë§ˆì§€ë§‰ í…ìŠ¤íŠ¸ ì„œëª… ìœ ì§€
            elif part.inline_data:
                result["images"].append(part.inline_data.data)
                image_sigs.append(sig)

        # ì •ë ¬ ë³´ì¥: [text_sig, img0_sig, img1_sig, ...]
        result["thought_signatures"] = [text_sig] + image_sigs

        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            result["prompt_tokens"] = getattr(response.usage_metadata, 'prompt_token_count', 0)
            result["candidates_tokens"] = getattr(response.usage_metadata, 'candidates_token_count', 0)

        return result

    # ============================================================
    # Batch API ë‹¤ì¤‘ ê²°ê³¼ ìƒì„± (50% í• ì¸)
    # ============================================================
    def chat_candidates(
        self,
        model: str,
        messages: List[ChatMessage],
        count: int = 1,
        on_job_created=None,
        **options
    ) -> list | None:
        """
        Gemini Batch API (client.batches.create)ë¡œ Nê°œ ìš”ì²­ì„ ë°°ì¹˜ ì²˜ë¦¬.

        ì¸ë¼ì¸ ìš”ì²­ ë°©ì‹ìœ¼ë¡œ Nê°œì˜ ë™ì¼ ìš”ì²­ì„ í•œ ë²ˆì— ì œì¶œ.
        í‘œì¤€ API ëŒ€ë¹„ 50% í• ì¸. ë¹„ë™ê¸° ì²˜ë¦¬ í›„ í´ë§ìœ¼ë¡œ ê²°ê³¼ ìˆ˜ì‹ .

        Args:
            model: ëª¨ë¸ ID (Imagen ì œì™¸ ëª¨ë“  ëª¨ë¸ ì§€ì›)
            messages: ëŒ€í™” ê¸°ë¡
            count: ìš”ì²­í•  ê²°ê³¼ ìˆ˜
            on_job_created: ì½œë°±(job_name, key_index) â€” job ìƒì„± ì§í›„ í˜¸ì¶œ (í ì €ì¥ìš©)
            **options: ëª¨ë¸ë³„ ì¶”ê°€ ì˜µì…˜

        Returns:
            list â€” ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (str: í…ìŠ¤íŠ¸ ëª¨ë¸, dict: ì´ë¯¸ì§€ ëª¨ë¸)
            None â€” ì‹¤íŒ¨ ì‹œ (callerê°€ ë³‘ë ¬ Workerë¡œ fallback)
        """
        if count <= 1:
            result = self.chat(model, messages, stream=False, **options)
            return [result] if result is not None else None

        # í”ŒëŸ¬ê·¸ì¸ ëª¨ë¸ ë””ìŠ¤íŒ¨ì¹˜
        from v.model_plugin import PluginRegistry
        plugin = PluginRegistry.instance().get_plugin_for_model(model)
        if plugin:
            return plugin.chat_candidates(model, messages, count, **options)

        self._cancel_requested = False

        # System instruction + contents ë¹Œë“œ
        sys_text = options.get("system_prompt", "")
        sys_files = options.get("system_files", [])
        sys_instr = self._build_system_instruction(sys_text, sys_files)

        client, key_index = self._get_client_with_index()
        contents = self._convert_messages(messages)

        # ëª¨ë¸ë³„ GenerateContentConfig ë¹Œë“œ
        is_nanobanana = model in ("gemini-3-pro-image-preview", "gemini-2.5-flash-image")

        config_kwargs = {"safety_settings": self._get_safety_settings()}
        if sys_instr:
            config_kwargs["system_instruction"] = sys_instr

        if is_nanobanana:
            config_kwargs["response_modalities"] = ["TEXT", "IMAGE"]
            config_kwargs["image_config"] = types.ImageConfig(
                aspect_ratio=options.get("aspect_ratio", "1:1"),
            )
        elif model in ("gemini-3.1-pro-preview", "gemini-3-pro-preview", "gemini-3-flash-preview"):
            config_kwargs["thinking_config"] = types.ThinkingConfig(
                thinking_level=options.get("thinking_level", "HIGH"),
            )
        elif model in ("gemini-2.5-pro", "gemini-2.5-flash"):
            budget = options.get("thinking_budget", 2804 if "pro" in model else 0)
            config_kwargs["thinking_config"] = types.ThinkingConfig(
                thinking_budget=budget,
            )

        # ê³µí†µ ìƒì„± ì˜µì…˜
        for key in ("temperature", "top_p", "max_output_tokens"):
            val = options.get(key)
            if val is not None:
                config_kwargs[key] = val

        config = types.GenerateContentConfig(**config_kwargs)

        # ì¸ë¼ì¸ ìš”ì²­ Nê°œ ë¹Œë“œ
        inline_requests = [
            {"contents": contents, "config": config}
            for _ in range(count)
        ]

        # Batch job ìƒì„±
        try:
            batch_job = client.batches.create(
                model=model,
                src=inline_requests,
                config={"display_name": f"qonvo-batch-{count}"},
            )
        except Exception:
            return None

        # í ì €ì¥ ì½œë°± (ë””ìŠ¤í¬ì— ì¦‰ì‹œ ì €ì¥)
        if on_job_created:
            try:
                on_job_created(batch_job.name, key_index)
            except Exception:
                pass

        # ì™„ë£Œê¹Œì§€ í´ë§ (5ì´ˆ ê°„ê²©)
        completed_states = {
            'JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED',
            'JOB_STATE_CANCELLED', 'JOB_STATE_EXPIRED',
        }
        while batch_job.state.name not in completed_states:
            if self._cancel_requested:
                try:
                    client.batches.cancel(name=batch_job.name)
                except Exception:
                    pass
                return None
            time.sleep(5)
            batch_job = client.batches.get(name=batch_job.name)

        if batch_job.state.name != 'JOB_STATE_SUCCEEDED':
            return None

        # ê²°ê³¼ ì¶”ì¶œ
        results = []
        for inline_resp in (batch_job.dest.inlined_responses or []):
            if not inline_resp.response:
                continue
            resp = inline_resp.response
            if is_nanobanana:
                results.append(self._parse_image_response(resp))
            else:
                text = ""
                for candidate in (resp.candidates or []):
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if getattr(part, 'thought', False):
                                continue
                            if part.text:
                                text += part.text
                if text:
                    results.append(text)

        return results if results else None

    # ============================================================
    # Batch job í´ë§ ì¬ê°œ (ì•± ì¬ì‹œì‘ í›„)
    # ============================================================
    def poll_batch_job(
        self,
        job_name: str,
        key_index: int,
        is_nanobanana: bool,
    ) -> list | None:
        """
        ê¸°ì¡´ batch jobì˜ í´ë§ ì¬ê°œ.

        ì•± ì¬ì‹œì‘ í›„ batch_queue.jsonì—ì„œ ì½ì€ job_nameìœ¼ë¡œ
        ê²°ê³¼ë¥¼ ìˆ˜ì‹ . ë™ì¼ API key(key_index)ë¡œ ì¡°íšŒ.

        Returns:
            list â€” ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (str ë˜ëŠ” dict)
            None â€” ì‹¤íŒ¨/ë§Œë£Œ/ì·¨ì†Œ
        """
        self._cancel_requested = False

        try:
            client = self._get_client_at(key_index)
        except (IndexError, RuntimeError) as e:
            from v.logger import get_logger
            get_logger("qonvo.provider").error(
                f"[BATCH_POLL] Client init failed for key_index={key_index}: {e}"
            )
            return None

        try:
            batch_job = client.batches.get(name=job_name)
        except Exception as e:
            from v.logger import get_logger
            get_logger("qonvo.provider").error(
                f"[BATCH_POLL] Failed to get batch job {job_name}: {e}"
            )
            return None

        # ì™„ë£Œê¹Œì§€ í´ë§ (5ì´ˆ ê°„ê²©)
        completed_states = {
            'JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED',
            'JOB_STATE_CANCELLED', 'JOB_STATE_EXPIRED',
        }
        while batch_job.state.name not in completed_states:
            if self._cancel_requested:
                try:
                    client.batches.cancel(name=job_name)
                except Exception:
                    pass
                return None
            time.sleep(5)
            try:
                batch_job = client.batches.get(name=job_name)
            except Exception:
                return None

        if batch_job.state.name != 'JOB_STATE_SUCCEEDED':
            return None

        # ê²°ê³¼ ì¶”ì¶œ
        results = []
        for inline_resp in (batch_job.dest.inlined_responses or []):
            if not inline_resp.response:
                continue
            resp = inline_resp.response
            if is_nanobanana:
                results.append(self._parse_image_response(resp))
            else:
                text = ""
                for candidate in (resp.candidates or []):
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if getattr(part, 'thought', False):
                                continue
                            if part.text:
                                text += part.text
                if text:
                    results.append(text)

        return results if results else None
